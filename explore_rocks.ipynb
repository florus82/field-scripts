{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import rasterio, glob, xarray as xr\n",
    "import os,sys\n",
    "import albumentations as A\n",
    "from albumentations.core.transforms_interface import  ImageOnlyTransform\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "sys.path.append(r'/home/repos')\n",
    "from torch.utils.data import DataLoader                                                                                 \n",
    "from tfcl.models.ptavit3d.ptavit3d_dn import ptavit3d_dn       \n",
    "from tfcl.nn.loss.ftnmt_loss import ftnmt_loss               \n",
    "from tfcl.utils.classification_metric import Classification  \n",
    "from datetime import datetime   \n",
    "from tqdm import tqdm\n",
    "from torch.amp import autocast, GradScaler\n",
    "import pandas as pd\n",
    "import random\n",
    "from rocksdbutils_copy import * \n",
    "from math import ceil as mceil\n",
    "import time\n",
    "\n",
    "# Set this to False for training\n",
    "#DEBUG=True\n",
    "DEBUG=False\n",
    "\n",
    "\n",
    "# Normalization and transform functions\n",
    "\n",
    "class AI4BNormal_S2(object):\n",
    "    \"\"\"\n",
    "    class for Normalization of images, per channel, in format CHW \n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "\n",
    "        self._mean_s2 = np.array([5.4418573e+02, 7.6761194e+02, 7.1712860e+02, 2.8561428e+03 ]).astype(np.float32) \n",
    "        self._std_s2  = np.array( [3.7141626e+02, 3.8981952e+02, 4.7989127e+02 ,9.5173022e+02]).astype(np.float32) \n",
    "\n",
    "    def __call__(self,img):\n",
    "\n",
    "        temp = img.astype(np.float32)\n",
    "        temp2 = temp.T\n",
    "        temp2 -= self._mean_s2\n",
    "        temp2 /= self._std_s2\n",
    "\n",
    "        temp = temp2.T\n",
    "        return temp\n",
    "\n",
    "class TrainingTransform_for_rocks_Train(object):\n",
    "    # Built on Albumentations, this provides geometric transformation only  \n",
    "    def __init__(self,  prob = 1, norm = AI4BNormal_S2()):\n",
    "        self.geom_trans = A.Compose([\n",
    "                  #  A.OneOf([\n",
    "                        A.HorizontalFlip(p=1),\n",
    "             #           A.VerticalFlip(p=1),\n",
    "            #            A.ElasticTransform(p=1), # VERY GOOD - gives perspective projection, really nice and useful - VERY SLOW   \n",
    "           #             A.GridDistortion(distort_limit=0.4,p=1),\n",
    "           #             A.ShiftScaleRotate(shift_limit=0.25, scale_limit=(0.75,1.25), rotate_limit=180, p=1.0), # Most important Augmentation   \n",
    "                #        ],p=1.)\n",
    "                    ],\n",
    "            #additional_targets={'imageS1': 'image','mask':'mask'},\n",
    "            p = prob)\n",
    "      \n",
    "        self.mytransform = self.transform_train\n",
    "        self.norm = norm\n",
    "        \n",
    "    def transform_valid(self, data):\n",
    "        timgS2, tmask = data\n",
    "        if self.norm is not None:\n",
    "            timgS2 = self.norm(timgS2)\n",
    "        \n",
    "        tmask= tmask \n",
    "        return timgS2,  tmask.astype(np.float32)\n",
    "\n",
    "    def transform_train(self, data):\n",
    "        timgS2, tmask = data\n",
    "        \n",
    "        if self.norm is not None:\n",
    "            timgS2 = self.norm(timgS2)\n",
    "\n",
    "        tmask= tmask \n",
    "        tmask = tmask.astype(np.float32)\n",
    "        # Special treatment of time series\n",
    "        c2,t,h,w = timgS2.shape\n",
    "        #print (c2,t,h,w)              \n",
    "        timgS2 = timgS2.reshape(c2*t,h,w)\n",
    "        result = self.geom_trans(image=timgS2.transpose([1,2,0]),\n",
    "                                 mask=tmask.transpose([1,2,0]))\n",
    "        timgS2_t = result['image']\n",
    "        tmask_t  = result['mask']\n",
    "        timgS2_t = timgS2_t.transpose([2,0,1])\n",
    "        tmask_t = tmask_t.transpose([2,0,1])\n",
    "        \n",
    "        c2t,h2,w2 = timgS2_t.shape\n",
    "\n",
    "        \n",
    "        timgS2_t = timgS2_t.reshape(c2,t,h2,w2)\n",
    "        return timgS2_t,  tmask_t\n",
    "    def __call__(self, *data):\n",
    "        return self.mytransform(data)\n",
    "\n",
    "class TrainingTransformS2(object):\n",
    "    # Built on Albumentations, this provides geometric transformation only  \n",
    "    def __init__(self,  prob = 1., mode='train', norm = AI4BNormal_S2()):\n",
    "        self.geom_trans = A.Compose([\n",
    "                    # A.RandomCrop(width=128, height=128, p=1.0),  # Always apply random crop\n",
    "                    # A.OneOf([\n",
    "                    #     A.HorizontalFlip(p=1),\n",
    "                    #     A.VerticalFlip(p=1),\n",
    "                    #     A.ElasticTransform(p=1), # VERY GOOD - gives perspective projection, really nice and useful - VERY SLOW   \n",
    "                    #     A.GridDistortion(distort_limit=0.4,p=1.),\n",
    "                    #     A.ShiftScaleRotate(shift_limit=0.25, scale_limit=(0.75,1.25), rotate_limit=180, p=1.0), # Most important Augmentation   \n",
    "                    #     ],p=1.)\n",
    "                    A.HorizontalFlip(p=1),\n",
    "                    A.VerticalFlip(p=1),\n",
    "                    A.ElasticTransform(p=1), # VERY GOOD - gives perspective projection, really nice and useful - VERY SLOW   \n",
    "                    A.GridDistortion(distort_limit=0.4,p=1.),\n",
    "                    A.ShiftScaleRotate(shift_limit=0.25, scale_limit=(0.75,1.25), rotate_limit=180, p=1.0), # Most important Augmentation   \n",
    "                    ],\n",
    "            additional_targets={'imageS1': 'image','mask':'mask'},\n",
    "            p = prob)\n",
    "        if mode=='train':\n",
    "            self.mytransform = self.transform_train\n",
    "        elif mode =='valid':\n",
    "            self.mytransform = self.transform_valid\n",
    "        else:\n",
    "            raise ValueError('transform mode can only be train or valid')\n",
    "            \n",
    "            \n",
    "        self.norm = norm\n",
    "        \n",
    "    def transform_valid(self, data):\n",
    "        timgS2, tmask = data\n",
    "        if self.norm is not None:\n",
    "            timgS2 = self.norm(timgS2)\n",
    "        \n",
    "        tmask= tmask \n",
    "        return timgS2,  tmask.astype(np.float32)\n",
    "\n",
    "    def transform_train(self, data):\n",
    "        timgS2, tmask = data\n",
    "        \n",
    "        if self.norm is not None:\n",
    "            timgS2 = self.norm(timgS2)\n",
    "\n",
    "        tmask= tmask \n",
    "        tmask = tmask.astype(np.float32)\n",
    "        # Special treatment of time series\n",
    "        c2,t,h,w = timgS2.shape\n",
    "        #print (c2,t,h,w)              \n",
    "        timgS2 = timgS2.reshape(c2*t,h,w)\n",
    "        result = self.geom_trans(image=timgS2.transpose([1,2,0]),\n",
    "                                 mask=tmask.transpose([1,2,0]))\n",
    "        timgS2_t = result['image']\n",
    "        tmask_t  = result['mask']\n",
    "        timgS2_t = timgS2_t.transpose([2,0,1])\n",
    "        tmask_t = tmask_t.transpose([2,0,1])\n",
    "        \n",
    "        c2t,h2,w2 = timgS2_t.shape\n",
    "\n",
    "        \n",
    "        timgS2_t = timgS2_t.reshape(c2,t,h2,w2)\n",
    "        return timgS2_t,  tmask_t\n",
    "    def __call__(self, *data):\n",
    "        return self.mytransform(data)\n",
    "\n",
    "# create a function to plot a numpy array\n",
    "def plotter(array):\n",
    "\n",
    "    # Plot the slices\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(20, 6), constrained_layout=False)  # 4 slices\n",
    "    slice_indices = np.linspace(0, array.shape[0] - 1, 4, dtype=int)\n",
    "\n",
    "    # Create a colormap\n",
    "    cmap = plt.cm.viridis\n",
    "\n",
    "    for ax, idx in zip(axes, slice_indices):\n",
    "        im = ax.imshow(array[idx, :, :], cmap=cmap)\n",
    "        ax.set_title(f\"Slice {idx}\")\n",
    "        ax.set_xticks([0, 32, 64, 96, 127])\n",
    "        ax.set_yticks([0, 32, 64, 96, 127])\n",
    "        ax.set_xticklabels(['X0', 'X32', 'X64', 'X96', 'X127'])\n",
    "        ax.set_yticklabels(['Y0', 'Y32', 'Y64', 'Y96', 'Y127'])\n",
    "\n",
    "        cbar_ax = ax.inset_axes([0.1, -0.2, 0.8, 0.05])  # [x, y, width, height]\n",
    "        cbar = fig.colorbar(im, cax=cbar_ax, orientation='horizontal')\n",
    "        cbar.set_label('Value Scale')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def getFilelist(originpath, ftyp):\n",
    "    files = os.listdir(originpath)\n",
    "    out   = []\n",
    "    for i in files:\n",
    "        if i.split('.')[-1] in ftyp:\n",
    "            if originpath.endswith('/'):\n",
    "                out.append(originpath + i)\n",
    "            else:\n",
    "                out.append(originpath + '/' + i)\n",
    "        # else:\n",
    "        #     print(\"non-matching file - {} - found\".format(i.split('.')[-1]))\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a rocksdb dataset\n",
    "random.seed(42)\n",
    "country = 'ES_no_empty_label'\n",
    "train_dataset = RocksDBDataset(f'/home/output/rocks_db/{country}.db/train.db')\n",
    "valid_dataset = RocksDBDataset(f'/home/output/rocks_db/{country}.db/valid.db') # when indexing this class no transform/normalisation will be performed unless specified else"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1210"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid_dataset)\n",
    "\n",
    "# aa = DataLoader(dataset=train_dataset, batch_size=2,\n",
    "#                               shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "# for i, e in aa:\n",
    "#     print(i.shape)\n",
    "#     print(e.shape)\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_no = 450\n",
    "# 450\n",
    "\n",
    "a = train_dataset[img_no][0]\n",
    "b = train_dataset[img_no][1]\n",
    "\n",
    "# apply transorm (normalization and augmentation)\n",
    "c, d = TrainingTransformS2(mode='train')(a, b)\n",
    "\n",
    "print(c.shape, d.shape)\n",
    "year = 5\n",
    "plotter(a[:, year, :, :])\n",
    "plotter(c[:, year, :, :])\n",
    "#plotter(a[:, year, :, :] - c[:,year,:,:])\n",
    "plotter(b)\n",
    "plotter(d)\n",
    "#plotter(b - d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see if there were empty label images from the beginning\n",
    "label_List, label_index_List = [], []\n",
    "for i, tupi in enumerate(train_dataset):\n",
    "    image = tupi[0]\n",
    "    label = tupi[1]\n",
    "\n",
    "    lc = np.unique(label)\n",
    "    \n",
    "    if len(lc) == 1:\n",
    "        label_List.append(lc)\n",
    "        label_index_List.append(i)\n",
    "\n",
    "\n",
    "# assess the label destruction trough the augmentation\n",
    "image_transformed_List, label_transformed_List = [], []\n",
    "image_transformed_index_List, label_transformed_index_List = [], []\n",
    "\n",
    "for i, tupi in enumerate(train_dataset):\n",
    "    image = tupi[0]\n",
    "    label = tupi[1]\n",
    "    \n",
    "    image_transformed, label_transformed = TrainingTransform_for_rocks_Train()(image, label)\n",
    "    #  ic = np.unique(image_transformed)\n",
    "    lc = np.unique(label_transformed)\n",
    "    # if len(ic) == 1:\n",
    "    #     image_transformed_List.append(ic)\n",
    "    #     image_transformed_index_List.append(i)\n",
    "    if len(lc) == 1:\n",
    "        label_transformed_List.append(lc)\n",
    "        label_transformed_index_List.append(i)\n",
    "\n",
    "# check the indices of labels that are 0 in the beginning and after transformation\n",
    "overlap = np.intersect1d(label_index_List, label_transformed_index_List)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_dataset))\n",
    "print(len(valid_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the rocksdb with 64x64 image chips for comparing train/valid ratio \n",
    "train_dataset64 = RocksDBDataset('/home/output/rocks_db/ES_64img.db/train.db')\n",
    "valid_dataset64 = RocksDBDataset('/home/output/rocks_db/ES_64img.db/valid.db')\n",
    "label_transformed_List64 = []\n",
    "label_transformed_index_List64 = []\n",
    "\n",
    "for i, tupi in enumerate(train_dataset64):\n",
    "    image = tupi[0]\n",
    "    label = tupi[1]\n",
    "    \n",
    "    image_transformed, label_transformed = TrainingTransformS2(mode='train')(image, label)\n",
    "    #  ic = np.unique(image_transformed)\n",
    "    lc = np.unique(label_transformed)\n",
    "    # if len(ic) == 1:\n",
    "    #     image_transformed_List.append(ic)\n",
    "    #     image_transformed_index_List.append(i)\n",
    "    if len(lc) == 1:\n",
    "        label_transformed_List64.append(lc)\n",
    "        label_transformed_index_List64.append(i)\n",
    "\n",
    "        \n",
    "label_List64, label_index_List64 = [], []\n",
    "for i, tupi in enumerate(train_dataset64):\n",
    "    image = tupi[0]\n",
    "    label = tupi[1]\n",
    "\n",
    "    lc = np.unique(label)\n",
    "    \n",
    "    if len(lc) == 1:\n",
    "        label_List64.append(lc)\n",
    "        label_index_List64.append(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country = 'ES'\n",
    "lenai4 = len(getFilelist(f'/home/ai4boundaries/sentinel2/masks/{country}/', 'tif')) -47\n",
    "lentrain = len(train_dataset) / lenai4\n",
    "lenvalid = len(valid_dataset) / lenai4\n",
    "\n",
    "print(f\"Exploration of results of the {country} database\\\n",
    "      \\nThe amount of images in ai4boundaries for '{country}' is {lenai4} images\\\n",
    "      \\n\\nThe parameters for creating rocks.db= stride_divisor = 2, batch_size = 2, train_split = 0.9\\\n",
    "      \\nWith this settings and a descrease in resoltuion from 256x256 to 128x128:\\\n",
    "      \\n--> one image from ai4boundaries is cutted into {int(lentrain + lenvalid)} chips; {int(lentrain)} go into \\\n",
    "      \\ntraining and {int(lenvalid)} into validation rocks database\")\n",
    "\n",
    "print(f\"\\nEmpty labels from trainings-dataset(n={len(train_dataset)}, for image chips 128x128):\\\n",
    "      \\nBefore transformation: {len(label_List)}\\nAfter transformation: {len(label_transformed_List)}\\\n",
    "      \\n\\n{(len(overlap)/len(label_List))*100}% of empty labels before transformation still empty after transformation\\\n",
    "      \\nThe ratio of validation to training in terms of dataset sizes is {round(len(valid_dataset)/len(train_dataset),2)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overlap64 = np.intersect1d(label_index_List64, label_transformed_index_List64)\n",
    "lenai64 = len(getFilelist('/home/ai4boundaries/sentinel2/masks/ES/', 'tif'))\n",
    "lentrain64 = len(train_dataset64) / lenai64\n",
    "lenvalid64 = len(valid_dataset64) / lenai64\n",
    "# qq = [lab for lab in label_index_List64 if lab in label_transformed_index_List64]\n",
    "# print(len(qq), len(label_index_List64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--------------------------------------------------------------------------------------------\")\n",
    "print(f\"\\nIf image is split into 64x64 chips:\\nEmpty labels from trainings-dataset(n={len(train_dataset64)})\\\n",
    "      \\nBefore transformation: {len(label_List64)}\\nAfter transformation: {len(label_transformed_List64)}\\\n",
    "      \\n\\n{round(len(overlap64)/len(label_List64),2)*100}% of empty labels before transformation still empty after transformation\\\n",
    "      \\nThe ratio of validation to training in terms of dataset sizes is {round(len(valid_dataset64)/len(train_dataset64),2)}\\\n",
    "      \\n\\nWith this settings from above and a descrease in resoltuion from 256x256 to 64x64:\\\n",
    "      \\n--> one image from ai4boundaries is cutted into {int(lentrain64 + lenvalid64)} chips; {int(lentrain64)} go into \\\n",
    "      \\ntraining and {int(lenvalid64)} into validation rocks database\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Speed considerations:\\nThe example notebook that does not use rocksdb needs ~ 165 s/it in an epoch with the 'ES' dataset\\\n",
    "      \\nUsing rocksdb for the same dataset results in ~ 927 s/it\\\n",
    "      \\nThis means, it needs {round(927/165,2)} times longer to use rocksdb.\\\n",
    "      \\nHowever, the notebook only takes one 128x128 image chip for training and one for validation, rocksdb uses 4 for training and ?4? for validation\\\n",
    "      \\nThis means, the speed might be the same, if not a bit faster for rocksdb, if all validation images were used\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search actual nc files for completely empty labels\n",
    "counter = 0\n",
    "mask_index0 = []\n",
    "masks = getFilelist(f'/home/ai4boundaries/sentinel2/masks/{country}/', 'tif')\n",
    "masks.sort()\n",
    "\n",
    "for i, img in enumerate(masks):\n",
    "        with rasterio.open(img) as src:\n",
    "            src_array = src.read()\n",
    "            if len(np.unique(src_array))==1:\n",
    "                counter += 1\n",
    "                mask_index0.append(i)\n",
    "                \n",
    "\n",
    "print(f\"\\n{counter} images from ai4boundaries have empty labels\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f\"However, as in the tif labels are only {counter} empty, there should be only {counter*4} empty labels in the training rocksdb dataset\\\n",
    "    \\nas there are 4 chips per image in this section of the dabase.\\\n",
    "    \\nLet's look into that:\"\n",
    "# indices in trainings rocksdb dataset that should be empty\n",
    "should_be_empty = list(np.array([[i*4, i*4+1, i*4+2, i*4+3] for i in mask_index0]).flat)\n",
    "for i in should_be_empty:\n",
    "    if len(np.unique(train_dataset[i][1])) != 1:\n",
    "        print(f\"Empty raw label through insertion into rocks now falsly all 0 at index {i}\")\n",
    "# test which other indices in rocksdb are now empty\n",
    "add0 = [i for i in label_index_List if i not in should_be_empty]\n",
    "print(len(add0))\n",
    "if len(label_index_List) - len(add0) != len(should_be_empty):\n",
    "    print(\"Something is wrong\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the indices from the additional zero labels to the indices in the ai4boundaries dataset\n",
    "raw_ind = [i//4 for i in add0]\n",
    "raw_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter(train_dataset[3][0][:,1,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the generate slice function\n",
    "\n",
    "def slicetester(shape, Fi, s):\n",
    "        \n",
    "        # Constants that relate to rows, columns \n",
    "        nTimesRows = int((shape[-2] - Fi)//s + 1)\n",
    "        nTimesCols = int((shape[-1] - Fi)//s + 1)\n",
    "\n",
    "        # Use these directly \n",
    "        RowsCols = [(row, col) for row in range(nTimesRows-1) for col in range(nTimesCols-1)]\n",
    "        RowsCols_Slices = [ (slice(row*s,row*s +Fi,1),slice(col*s,col*s+Fi,1) )  for (row,col) in RowsCols ]\n",
    "        #print(RowsCols)\n",
    "        #print(RowsCols_Slices)\n",
    "        # Construct RowsCols for last Col \n",
    "        col_rev = shape[-1]-Fi\n",
    "        Rows4LastCol = [(row,col_rev) for row in range(nTimesRows-1)]\n",
    "        Rows4LastCol_Slices = [ (slice(row*s,row*s +Fi,1),slice(col_rev,col_rev+Fi,1) )  for (row,col_rev) in Rows4LastCol]\n",
    "\n",
    "        # Construct RowsCols for last Row \n",
    "        row_rev = shape[-2]-Fi\n",
    "        Cols4LastRow        = [(row_rev,col) for col in range(nTimesCols-1)]\n",
    "        Cols4LastRow_Slices = [(slice(row_rev,row_rev+Fi,1),slice(col*s,col*s +Fi,1) )  for (row_rev,col) in Cols4LastRow]\n",
    "\n",
    "        \n",
    "        # Store all Rows and Columns that correspond to raster slices and slices \n",
    "        RowsCols           = RowsCols + Rows4LastCol + Cols4LastRow\n",
    "        RowsCols_Slices    = RowsCols_Slices + Rows4LastCol_Slices + Cols4LastRow_Slices\n",
    "\n",
    "        return RowsCols, RowsCols_Slices\n",
    "\n",
    "q,w = slicetester((3, 256, 256), 64, 32)\n",
    "for i in q:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test batchify\n",
    "\n",
    "def batchitester(batch_size, RowsCols, RowsCols_Slices):\n",
    "    n = mceil(len(RowsCols)/batch_size)\n",
    "    BatchIndices  = np.array_split(list(range(len(RowsCols))),n,axis=0)\n",
    "    BatchRowsCols = np.array_split(RowsCols,n,axis=0)\n",
    "    BatchRowsCols_Slices = np.array_split(RowsCols_Slices,n,axis=0)\n",
    "\n",
    "    return BatchIndices, BatchRowsCols, BatchRowsCols_Slices\n",
    "\n",
    "a, s, d = batchitester(2, q, w)\n",
    "print(a)\n",
    "print(s)\n",
    "print(d)\n",
    "\n",
    "print(int(0.9*len(s)))\n",
    "for r, c in d:\n",
    "    print(r, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1, w1 = valid_dataset[0]\n",
    "q2, w2 = valid_dataset[1]\n",
    "q3, w3 = valid_dataset[2]\n",
    "\n",
    "plotter(w1)\n",
    "plotter(w2)\n",
    "plotter(w3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
