{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a06134",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.error\n",
    "import urllib.request\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# URL of data set\n",
    "url = 'http://jeodpp.jrc.ec.europa.eu/ftp/jrc-opendata/DRLL/AI4BOUNDARIES/'\n",
    "\n",
    "\n",
    "def download_file(url, dst_path):\n",
    "    \"\"\"\n",
    "    Download files to disk\n",
    "\n",
    "    :param url: URL of the file to download\n",
    "    :param dst_path: File location on disk after download\n",
    "\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with urllib.request.urlopen(url) as web_file:\n",
    "            data = web_file.read()\n",
    "            with open(dst_path, mode='wb') as local_file:\n",
    "                local_file.write(data)\n",
    "    except urllib.error.URLError as e:\n",
    "        print(e)\n",
    "\n",
    "\n",
    "def download_ai4boundaries(dir):\n",
    "    \"\"\"\n",
    "    Download AI4boundaries data set\n",
    "    :param dir: Path to directory where to save the data\n",
    "\n",
    "    \"\"\"\n",
    "    url = 'http://jeodpp.jrc.ec.europa.eu/ftp/jrc-opendata/DRLL/AI4BOUNDARIES/'\n",
    "    urls = []\n",
    "    url_fns = []\n",
    "\n",
    "    def scrape(site):\n",
    "        \"\"\"\n",
    "        Recursively scrape a website\n",
    "        :param site:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        # getting the request from url\n",
    "        r = requests.get(site)\n",
    "\n",
    "        # converting the text\n",
    "        s = BeautifulSoup(r.text, \"html.parser\")\n",
    "\n",
    "        for i in s.find_all(\"a\"):\n",
    "            href = i.attrs['href']\n",
    "\n",
    "            if href.endswith(\"/\"):\n",
    "\n",
    "                subsite = site + href\n",
    "\n",
    "                if subsite not in urls:\n",
    "                    urls.append(subsite)\n",
    "\n",
    "                    # calling it self\n",
    "                    scrape(subsite)\n",
    "            if href.endswith(\"tif\") | href.endswith(\"nc\"):\n",
    "                url_fn_ = site + href\n",
    "                url_fns.append(url_fn_)\n",
    "\n",
    "    print('Scraping data')\n",
    "    scrape(url)\n",
    "\n",
    "    print('Creating folder architecture')\n",
    "    if dir.endswith('/'):\n",
    "        subdirs = [i.replace(url, dir) for i in urls if not i.endswith('DRLL/')]\n",
    "    else:\n",
    "        subdirs = [i.replace(url, dir + '/') for i in urls if not i.endswith('DRLL/')]\n",
    "\n",
    "    subdirs = [subdir.replace('DRLL/', '') for subdir in subdirs if not 'ftp' in subdir]\n",
    "\n",
    "    for subdir in subdirs:\n",
    "        Path(subdir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    failed_fns = []\n",
    "    print('Downloading data')\n",
    "    for url_fn in tqdm(url_fns):\n",
    "        if dir.endswith('/'):\n",
    "            fn = url_fn.replace(url, dir)\n",
    "        else:\n",
    "            fn = url_fn.replace(url, dir + '/')\n",
    "        try:\n",
    "            download_file(url_fn, fn)\n",
    "        except:\n",
    "            time.sleep(20)\n",
    "            failed_fns = url_fn\n",
    "\n",
    "    # Reprocessing failed downloads\n",
    "    for url_fn in tqdm(failed_fns):\n",
    "        if dir.endswith('/'):\n",
    "            fn = url_fn.replace(url, dir)\n",
    "        else:\n",
    "            fn = url_fn.replace(url, dir + '/')\n",
    "        try:\n",
    "            download_file(url_fn, fn)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    print('Download finished!')\n",
    "    print('Cite the data set:')\n",
    "    print('d\\'Andrimont, R., Claverie, M., Kempeneers, P., Muraro, D., Yordanov, M., Peressutti, D., Batiƒç, M., '\n",
    "          'and Waldner, F.: AI4Boundaries: an open AI-ready dataset to map field boundaries with Sentinel-2 and aerial '\n",
    "          'photography, Earth Syst. Sci. Data Discuss. [preprint], '\n",
    "          'https://doi.org/10.5194/essd-2022-298, in review, 2022.')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42678a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    out_dir = path_safe('/data/Aldhani/eoagritwin/fields/')\n",
    "    download_ai4boundaries(out_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai4_down",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
