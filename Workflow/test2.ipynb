{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fb4d1ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:albumentations.check_version:A new version of Albumentations is available: 2.0.8 (you have 1.4.13). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('../../other_repos/')\n",
    "sys.path.append('../../helperToolz/')\n",
    "from helper import *\n",
    "from helpsters import * \n",
    "\n",
    "# sys.path.append('/home/potzschf/repos/')\n",
    "# from helperToolz.helpsters import *\n",
    "\n",
    "\n",
    "####################################################### Prepare\n",
    "\n",
    "# make vrts from force outputs for easier processing\n",
    "year = 2023\n",
    "reduced_files = reduce_forceTSA_output_to_validmonths(f'/data/Aldhani/eoagritwin/force/output/BRANDENBURG/{year}/', 3, 8)\n",
    "# ordered_files = force_order_Colors_for_VRT(reduced_files, ['BLU', 'GRN', 'RED', 'BNR'], [f'MONTH-{d:02d}' for d in range(3,9,1)])\n",
    "# force_to_vrt(reduced_files, ordered_files, f'/data/Aldhani/eoagritwin/fields/Auxiliary/vrt/{year}/', True, bandnames=['BLU', 'GRN', 'RED', 'BNR'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7c5313",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "dirname() missing 1 required positional argument: 'p'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdirname\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: dirname() missing 1 required positional argument: 'p'"
     ]
    }
   ],
   "source": [
    "/home/fields/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c368b1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_forcefiles=reduced_files\n",
    "ordered_forcetiles=ordered_files,\n",
    "vrt_out_path=f'/data/Aldhani/eoagritwin/fields/Auxiliary/vrt/{year}/'\n",
    "pyramids=True\n",
    "bandnames=['BLU', 'GRN', 'RED', 'BNR']\n",
    "\n",
    "    \n",
    "# tiles = list(set([file.split('output/')[-1].split('/')[1].split('/')[0] for file in list_of_forcefiles]))\n",
    "force_folder_name = getFORCExyRangeName(get_forcetiles_range(list_of_forcefiles))\n",
    "if not vrt_out_path.endswith('/'):\n",
    "    vrt_out_path = vrt_out_path + '/'\n",
    "outDir = f'{vrt_out_path}{force_folder_name}/'\n",
    "if not os.path.exists(outDir):\n",
    "    os.makedirs(outDir)\n",
    "    print(outDir)\n",
    "    vrts = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284aa2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(ordered_forcetiles)):\n",
    "        vrt_name = f'{outDir}{force_folder_name}_{str(i)}.vrt'\n",
    "        vrt = gdal.BuildVRT(vrt_name, ordered_forcetiles[i], separate = False)\n",
    "        vrt = None\n",
    "\n",
    "        # make paths in vrts relative\n",
    "        convertVRTpathsTOrelative(vrt_name)\n",
    "        vrts.append(vrt_name)\n",
    "\n",
    "# set optionally bandnames    \n",
    "if bandnames:\n",
    "    for idz, bname in enumerate(bandnames): \n",
    "        print(f'{outDir}{force_folder_name}_{str(idz)}.vrt')\n",
    "        vrt = gdal.Open(f'{outDir}{force_folder_name}_{str(idz)}.vrt', gdal.GA_Update)  # VRT must be writable\n",
    "        band = vrt.GetRasterBand(1)\n",
    "        band.SetDescription(bname)\n",
    "        vrt = None\n",
    "print('single vrts created')\n",
    "    \n",
    "nums = [int(vrt.split('_')[-1].split('.')[0]) for vrt in vrts]\n",
    "vrts_sorted = sortListwithOtherlist(nums, vrts)[-1]\n",
    "print('paths in vrts made relative')\n",
    "\n",
    "vrt = gdal.BuildVRT(f'{outDir}{force_folder_name}_Cube.vrt', vrts_sorted, separate = True)\n",
    "vrt = None\n",
    "if bandnames:\n",
    "    # set vrt band names\n",
    "    vrt = gdal.Open(f'{outDir}{force_folder_name}_Cube.vrt', gdal.GA_Update)  # VRT must be writable\n",
    "    for idz, bname in enumerate(bandnames): \n",
    "        band = vrt.GetRasterBand(1+idz)\n",
    "        band.SetDescription(bname)\n",
    "    vrt = None\n",
    "# convertVRTpathsTOrelative(f'{outDir}{force_folder_name}_Cube.vrt')\n",
    "print('overlord vrt created')\n",
    "if pyramids:\n",
    "    # build pyramids\n",
    "    vrtPyramids(f'{outDir}{force_folder_name}_Cube.vrt')\n",
    "    print('VRT created with pyramids')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a182d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load vrts into npdstack\n",
    "dat = loadVRTintoNumpyAI4(f'/data/fields/Auxiliary/vrt/{year}/{getFORCExyRangeName(get_forcetiles_range(reduced_files))}')\n",
    "\n",
    "# set tiling scheme and chip size on which prediction will be undertaken\n",
    "chipsize = 128*1 # 5 is the maximum with GPU in basement\n",
    "overlap  = 20\n",
    "\n",
    "row_col_ind = get_row_col_indices(chipsize, overlap, dat.shape[2:][0], dat.shape[2:][1])\n",
    "\n",
    "####################################################### Predict\n",
    "\n",
    "predicted_chips_list = predict_on_GPU('/data/fields/output/model_state_All_but_LU_transformed_42.pth', row_col_ind, dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7443f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "'/data/fields/output/rocks_db/AI4_RGB_NDVI_exclude_false.db'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bae2108",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import ceil as mceil\n",
    "\n",
    "img_size = 128\n",
    "## create metadata file\n",
    "metadata = {\n",
    "    'inputs': {\n",
    "        'inputs_shape': (5, 6, img_size, img_size),  \n",
    "        'inputs_dtype': np.float32     \n",
    "    },\n",
    "    'labels': {\n",
    "        'labels_shape': (4, img_size, img_size),          \n",
    "        'labels_dtype': np.float32\n",
    "    }\n",
    "}\n",
    "\n",
    "## define function to load imgs and labs\n",
    "def names2array_function(names):\n",
    "\n",
    "    variables2use=['B2','B3','B4','B8','NDVI']\n",
    "\n",
    "    image_path, label_path = names\n",
    "    # load image\n",
    "    img = xr.open_dataset(image_path)\n",
    "    image = np.concatenate([img[var].values[None] for var in variables2use],0)\n",
    "    \n",
    "    # load label\n",
    "    ds = xr.open_dataset(label_path)\n",
    "    label = np.asarray(ds['band_data'].values)\n",
    "    label[np.isnan(label)] = 0\n",
    "    return [image, label] \n",
    "\n",
    "def names2array_function16(names):\n",
    "\n",
    "    variables2use=['B2','B3','B4','B8']#,'NDVI']\n",
    "\n",
    "    image_path, label_path = names\n",
    "    # load image\n",
    "    img = xr.open_dataset(image_path)\n",
    "    image = np.concatenate([img[var].values[None] for var in variables2use],0)\n",
    "    \n",
    "    # load label\n",
    "    ds = xr.open_dataset(label_path)\n",
    "    label = np.asarray(ds['band_data'].values)\n",
    "    label[np.isnan(label)] = 0\n",
    "    return [image.astype(np.float16), label.astype(np.float16)]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f616be",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create list of images and labels\n",
    "\n",
    "folders = ['LU', 'AT', 'ES', 'FR', 'NL', 'SE', 'SI']\n",
    "imgs = [getFilelist('/home/ai4boundaries/sentinel2/images/' + folder, '.nc') for folder in folders]\n",
    "imgs = [img for list in imgs for img in list]\n",
    "labs = [getFilelist('/home/ai4boundaries/sentinel2/masks/' + folder, '.tif') for folder in folders]\n",
    "labs = [lab for list in labs for lab in list]\n",
    "print(len(imgs), len(labs))\n",
    "imgs.sort()\n",
    "labs.sort()\n",
    "\n",
    "## exclude completely empty images\n",
    "exclude = False\n",
    "if exclude == True:\n",
    "    aa = checkemptyNC(labs)\n",
    "    # exclude images with empty labels and their labels\n",
    "    labs = [lab for i, lab in enumerate(labs) if i not in aa]\n",
    "    imgs = [img for i, img in enumerate(imgs) if i not in aa]\n",
    "\n",
    "print(len(imgs), len(labs))\n",
    "\n",
    "\n",
    "img_lab_paths = []\n",
    "for i in range(len(imgs)):\n",
    "    img_lab_paths.append((imgs[i], labs[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077edb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from skimage import measure\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "import sys\n",
    "sys.path.append('/home/potzschf/repos/')\n",
    "from helperToolz.helpsters import *\n",
    "\n",
    "\n",
    "#########################  parameter settings\n",
    "# load the predictions and labels\n",
    "\n",
    "reference  = '/data/fields/IACS/4_Crop_mask/GSA-DE_BRB-2019_cropMask_lines_touch_true_lines_touch_true_linecrop.tif'\n",
    "    \n",
    "predictions =  '/data/fields/output/predictions/FORCE/BRANDENBURG/vrt/256_20_chips.vrt' # predictions straight from GPU \n",
    "#reference =  '/data/fields/IACS/4_Crop_mask/GSA-DE_BRB-2019_cropMask_lines_touch_true_lines_touch_true_linecrop.tif' # mask from IACS\n",
    "result_dir = '/data/fields/Auxiliary/grid_search/Brandenburg/' + predictions.split('/')[-1].split('.')[0] + '_masked_with_and_preds_are_' + reference.split('/')[-1].split('.')[0]\n",
    "sub = predictions.split('/')[-1].split('.')[0] + '_masked_with_and_preds_are_' + reference.split('/')[-1].split('.')[0]\n",
    "folder_path = f'/data/fields/output/predictions/FORCE/BRANDENBURG/auxiliary/{sub}/'\n",
    "vrt_for_folder_path = folder_path + 'vrt/'\n",
    "os.makedirs(folder_path, exist_ok=True)\n",
    "os.makedirs(vrt_for_folder_path, exist_ok=True)\n",
    "os.makedirs(result_dir, exist_ok=True)\n",
    "# set the number by which rows and cols will be divided --> determines the number of tiles // also set border limit (dont sample fields too close to tile borders) and sample size\n",
    "slicer = 10\n",
    "border_limit = 5\n",
    "sample_size  = 10000\n",
    "# set the number of cores for parallel processing and set seed\n",
    "ncores = 20\n",
    "np.random.seed(42)\n",
    "make_tifs_from_intermediate_step = True\n",
    "######### prepare job-list\n",
    "\n",
    "\n",
    "# create lists that will be passed on to the joblist\n",
    "tile_list = []\n",
    "extent_true_list = []\n",
    "extent_pred_list = []\n",
    "boundary_pred_list = []\n",
    "result_dir_list = []\n",
    "row_col_start = []\n",
    "\n",
    "\n",
    "# tile predictions in prds --> total extent encompasses 90 Force Tiles (+ a few rows and cols that will be neglected as they are outside of study area)\n",
    "pred_ds = gdal.Open(predictions)\n",
    "rows, cols = pred_ds.RasterYSize, pred_ds.RasterXSize\n",
    "\n",
    "row_start = [i for i in range(0, rows, math.floor(rows/slicer))]\n",
    "row_end = [i for i in range (math.floor(rows/slicer), rows, math.floor(rows/slicer))]\n",
    "row_start = row_start[:len(row_end)] \n",
    "\n",
    "col_start = [i for i in range(0, cols, math.floor(cols/slicer))]\n",
    "col_end = [i for i in range (math.floor(cols/slicer), cols, math.floor(cols/slicer))]\n",
    "col_start = col_start[:len(col_end)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c54035e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load IACS reference mask and label it \n",
    "ref_ds = gdal.Open(reference)\n",
    "extent_true = ref_ds.GetRasterBand(1).ReadAsArray()\n",
    "binary_true = extent_true > 0\n",
    "instances_true = measure.label(binary_true, background=0, connectivity=1)\n",
    "\n",
    "# sample fields\n",
    "# build a mask to exclude fields that are in border_limit to tile borders\n",
    "power_mask = np.zeros(instances_true.shape)\n",
    "for i in range(len(row_end)):\n",
    "    for j in range(len(col_end)):\n",
    "            power_mask[row_start[i]:row_start[i] + border_limit, :] = 1\n",
    "            power_mask[:, col_start[j]:col_start[j] + border_limit] = 1\n",
    "            power_mask[row_end[-1] - border_limit:power_mask.shape[0], :] = 1\n",
    "            power_mask[:, col_end[-1] - border_limit:power_mask.shape[1]] = 1\n",
    "\n",
    "# get IDs from labelled reference\n",
    "IDs_to_skip = np.unique(instances_true[power_mask==1])\n",
    "\n",
    "# get distribution of field sizes after segmentation\n",
    "unique_IDs, counts = np.unique(instances_true, return_counts=True)\n",
    "\n",
    "# exlcude fields that are too close to tile borders\n",
    "mask = ~np.isin(unique_IDs, IDs_to_skip)\n",
    "unique_IDs = unique_IDs[mask]\n",
    "counts = counts[mask]\n",
    "\n",
    "# exlude 0 (background) and 1 (super-small fields) from sample\n",
    "mask = (unique_IDs != 0) & (counts > 2)\n",
    "unique_IDs = unique_IDs[mask]\n",
    "counts = counts[mask]\n",
    "\n",
    "\n",
    "# get deciles and draw equally from them\n",
    "deciles = [perc for perc in range(10,100,10)]\n",
    "deciles_values = np.percentile(counts, deciles)\n",
    "decs = [0] + deciles_values.tolist() + [np.max(counts)]\n",
    "bin_ids = []\n",
    "for ind in range(len(decs) -1):\n",
    "    # get the unique_IDS of those fields, whose count (size) is within bin\n",
    "    bin_ids.append(np.random.choice(unique_IDs[(counts > decs[ind]) & (counts <= decs[ind + 1])], int(sample_size/10), replace=False))\n",
    "\n",
    "mask = np.isin(instances_true, np.concatenate(bin_ids))\n",
    "# set everything to 0 except samples\n",
    "instances_true[~mask] = 0\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777d0bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "j=6\n",
    "# read in vrt in tiles\n",
    "\n",
    "    \n",
    "######### fill the lists with tiled data\n",
    "\n",
    "\n",
    "#subset the prediction of fields read-in\n",
    "extent_pred = pred_ds.GetRasterBand(1).ReadAsArray(col_start[j], row_start[i], col_end[j] - col_start[j], row_end[i] - row_start[i]) # goes into InstSegm --> image of crop probability \n",
    "# mask extend_pred with reference\n",
    "extent_pred_masked = extent_pred * extent_true[row_start[i]:row_end[i], col_start[j]:col_end[j]]\n",
    "\n",
    "\n",
    "# check if tile contains a sample of reference/label data\n",
    "extent_true_label = instances_true[row_start[i]:row_end[i], col_start[j]:col_end[j]]\n",
    "\n",
    "extent_true_list.append(extent_true_label)\n",
    "extent_pred_list.append(extent_pred_masked)\n",
    "\n",
    "# make identifier for tile for csv\n",
    "tile_list.append(f'{str(i)}_{str(j)}')\n",
    "# load predicted boundary prob subset // goes into InstSegm --> image of boundary probability\n",
    "boundary_pred_list.append(pred_ds.GetRasterBand(2).ReadAsArray(col_start[j], row_start[i], col_end[j] - col_start[j], row_end[i] - row_start[i])) \n",
    "# output folder\n",
    "result_dir_list.append(result_dir)\n",
    "row_col_start.append(str(row_start[i]) + '_' + str(col_start[j]))\n",
    "\n",
    "jobs = [[tile_list[i], row_col_start[i] ,extent_true_list[i], extent_pred_list[i], boundary_pred_list[i], result_dir_list[i],  pred_ds.GetGeoTransform(), pred_ds.GetProjection(), folder_path, border_limit]  for i in range(len(result_dir_list))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7747b78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = 0\n",
    "\n",
    "row_col_start = jobs[ind][1]\n",
    "extent_true = jobs[ind][2]\n",
    "extent_pred = jobs[ind][3]\n",
    "boundary_pred = jobs[ind][4]\n",
    "t_ext=0.2\n",
    "t_bound=0.2\n",
    "border_limit=border_limit = jobs[ind][6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ccc617",
   "metadata": {},
   "outputs": [],
   "source": [
    "row_start = int(row_col_start.split('_')[0])\n",
    "col_start = int(row_col_start.split('_')[1])\n",
    "# get predicted instance segmentation\n",
    "instances_pred = InstSegm(extent_pred, boundary_pred, t_ext=t_ext, t_bound=t_bound)\n",
    "instances_pred = measure.label(instances_pred, background=-1) \n",
    "instances_true = extent_true\n",
    "field_values = np.unique(instances_true)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ccb505",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc306fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_IoUs = []\n",
    "field_IDs = []\n",
    "field_sizes = []\n",
    "centroid_rows = []\n",
    "centroid_cols = []\n",
    "centroid_IoUS = []\n",
    "centroid_IDs = []\n",
    "intersectL  = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c491dcf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for field_value in field_values:\n",
    "#     if field_value == 0:\n",
    "#         continue # move on to next value\n",
    "\n",
    "field_value = field_values[1]\n",
    "this_field = instances_true == field_value\n",
    "# # check if field is close to border and throw away if too close\n",
    "# if TooCloseToBorder(this_field, border_limit):\n",
    "#     continue\n",
    "\n",
    "# calculate centroid\n",
    "this_field_centroid = np.mean(np.column_stack(np.where(this_field)),axis=0).astype(int)\n",
    "\n",
    "# fill lists with info\n",
    "centroid_rows.append(this_field_centroid[0])\n",
    "centroid_cols.append(this_field_centroid[1])\n",
    "field_IDs.append(field_value)\n",
    "field_sizes.append(np.sum(this_field))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb427da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(field_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7a7fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "this_field_centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfe5b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# find predicted fields that intersect with true field\n",
    "intersecting_fields = this_field * instances_pred\n",
    "intersect_values = np.unique(intersecting_fields)\n",
    "\n",
    "# compute IoU for each intersecting field\n",
    "field_IoUs = []\n",
    "center_IoU = 0\n",
    "for intersect_value in intersect_values:\n",
    "    if intersect_value == 0:\n",
    "        field_IoUs.append(0)\n",
    "        continue # move on to next value\n",
    "    \n",
    "    pred_field = instances_pred == intersect_value\n",
    "    union = this_field + pred_field > 0\n",
    "    intersection = (this_field * pred_field) > 0\n",
    "    IoU = np.sum(intersection) / np.sum(union)\n",
    "    field_IoUs.append(IoU)\n",
    "    # check for centroid condition\n",
    "    if instances_pred[this_field_centroid[0], this_field_centroid[1]] == intersect_value:\n",
    "        center_IoU = IoU\n",
    "        centroid_IDs.append(field_value)\n",
    "\n",
    "# take maximum IoU - this is the IoU for this true field\n",
    "if len(field_IoUs) != 0:\n",
    "    best_IoUs.append(np.max(field_IoUs))\n",
    "    # fill centroid list\n",
    "    centroid_IoUS.append(center_IoU)\n",
    "    max_index = np.argmax(field_IoUs)\n",
    "    intersectL.append(intersect_values[max_index])\n",
    "\n",
    "else:\n",
    "    best_IoUs.append(0)\n",
    "    # fill centroid list\n",
    "    centroid_IoUS.append(0)\n",
    "\n",
    "\n",
    "# Create mask of intersecting fields with best IoUs\n",
    "intersect_mask = np.isin(instances_pred, intersectL)\n",
    "filtered_instances_pred = instances_pred * intersect_mask\n",
    "# centroids\n",
    "for r,c, cid in zip(centroid_rows, centroid_cols, centroid_IDs):\n",
    "filtered_instances_pred[r, c] = cid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70cbe0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "intersecting_fields.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
